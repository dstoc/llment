# llm
Trait-based LLM client implementations for multiple providers.

## Dependencies
- async-trait
  - async trait abstraction
- serde_json
  - schema sanitization and parsing
- tokio-stream
  - stream response handling
- clap
  - parse provider enum for CLI usage
- ollama-rs (dstoc fork)
  - communicate with Ollama using streaming and tools
- async-openai
  - connect to OpenAI models
- openai-harmony (v0.0.4, git tag)
  - render Harmony prompts and parse responses for gpt-oss
- gemini-rs
  - connect to Gemini models
- rmcp
  - connect to MCP servers
- schemars
  - define and manipulate tool schemas

## Features
- LLM clients
  - `LlmClient` trait streams chat responses and lists supported model names
- implementations for Ollama, OpenAI, GptOss, and Gemini
- GptOss client uses v1/completions with Harmony format for `gpt-oss`
  - sends raw token arrays to the llama-server endpoint
- GptOss client builds prompts via helper that handles thinking, final, or both segments when the last history message is from the assistant and emits optional prefills accordingly
  - analysis segments preceding final content are omitted from prompts unless the final message is prefilled
  - streaming parser is primed with prefill tokens so continuation in the same channel is captured
  - tool calls render in the commentary channel with constrained JSON
  - tool responses map to tool role messages in the commentary channel addressed to the assistant
- Provider selection
  - `Provider` enum lists supported backends
  - `client_from` builds a client for the given provider and model
    - stores provider and model names for later retrieval
- Tool schemas
  - `to_openapi_schema` strips `$schema` and converts unsigned ints to signed formats
- Core message and tool types defined locally instead of re-exporting from `ollama-rs`
  - tool calls hold name and arguments directly and preserve unparseable argument strings
  - tool info stores name, description, and parameters without wrapper enums
  - chat messages are an enum of `UserMessage`, `AssistantMessage`, `SystemMessage`, and `ToolMessage`, each with only relevant fields
    - tool calls include an `id` string, assigned locally when missing
    - tool messages carry the same `id` and store `content` as `serde_json::Value`
- Chat message, request, and response types serialize to and from JSON
  - skips serializing fields that are `None`, empty strings, or empty arrays
- Responses
  - `ResponseChunk` is an enum of `Thinking`, `ToolCall`, `Content`, `Usage`, or `Done`
  - usage chunks carry `input_tokens` and `output_tokens`
  - tool call chunks hold a single `ToolCall` and repeat as needed
  - thinking, tool calls, and content stream first, followed by optional usage then `Done`
  - OpenAI client converts assistant history messages with tool calls into request `tool_calls` and stitches streaming tool call deltas into complete tool calls
  - OpenAI client parses `reasoning_content` from streamed responses into thinking text
- Tool orchestration
  - `tools` module exposes a `ToolExecutor` trait
  - `run_tool_loop` streams responses, executes tools, and issues follow-up requests
  - `tool_event_stream` spawns the loop and yields `ToolEvent`s
    - `RequestStarted` fires when a new request is sent
    - join handle resolves on completion with history updated in place
    - `ToolStarted` events include original argument strings when parsing fails
    - `ToolStarted` and `ToolResult` keep the original `ToolCall` id
  - tool calls with invalid arguments skip executor invocation and return "Could not parse arguments as JSON"
- `mcp` module
- `load_mcp_servers` starts configured MCP servers and collects tool schemas
  - tool names are prefixed with the server name
  - `McpService` implements `ClientHandler`
    - `on_tool_list_changed` refreshes tool metadata from the service
    - tool metadata stored in an `ArcSwap` for lock-free snapshots
  - `McpContext` stores running service handles keyed by prefix
    - supports runtime insertion and removal of services via internal locking
    - exposes merged `tool_infos` from all services
    - provides a non-blocking `tool_names` snapshot of available tools
    - implements `ToolExecutor` for MCP calls
      - unrecognized or unprefixed tool names return "{name} is not a valid tool name"
    - tool call chunks insert assistant messages immediately before execution
      - accumulated assistant content is flushed as a separate assistant message
      - the tool-call assistant message carries empty `content` with `tool_calls` populated
    - accumulated streamed content is appended as an assistant message after the stream completes
- Test utilities
  - `TestProvider` implements `LlmClient`
    - captures `ChatMessageRequest`s for assertions
    - streams queued `ResponseChunk`s for iterative testing

## Constraints
- uses provider-specific default host when none is supplied
- GptOss defaults to `http://localhost:8000/v1` and supports only `gpt-oss` via v1/completions
- deprecated `function_call` streaming is no longer supported
