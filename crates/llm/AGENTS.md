# llm
Trait-based LLM client implementations for multiple providers.

## Dependencies
- async-trait
  - async trait abstraction
- serde_json
  - schema sanitization and parsing
- tokio-stream
  - stream response handling
- clap
  - parse provider enum for CLI usage
- ollama-rs (dstoc fork)
  - communicate with Ollama using streaming and tools
- async-openai
  - connect to OpenAI models
- reqwest
  - direct llama-server HTTP calls
- reqwest-eventsource
  - handle server-sent events from llama-server
- openai-harmony (v0.0.4, git tag)
  - render Harmony prompts and parse responses for gpt-oss
- gemini-rust
  - connect to Gemini models
- rmcp
  - connect to MCP servers
- schemars
  - define and manipulate tool schemas
- gbnf-rs
  - generate GBNF grammars from tool schemas

## Features
- LLM clients
- `LlmClient` trait streams chat responses and lists supported model names
  - implementations for Ollama, OpenAiChat, Harmony, and GeminiRust
  - GeminiRust function responses place success data under an `output` field and errors under an `error` field
- Harmony client uses `/completion` with Harmony format for `gpt-oss`
  - sends raw token arrays via `llama_server_completion`
  - sends a GBNF grammar that constrains tool call JSON to each tool's schema
  - grammar base loaded from `.gbnf` file with `include_str!` and no root rule
    - appends `root ::= harmony` or `root ::= harmony_prefill_*` based on prefill context
  - tool-call rule appended to allow only declared tool names
  - Harmony client builds prompts via helper that handles thinking, final, or both segments when the last history message is from the assistant and emits optional prefills accordingly
  - analysis segments preceding final content are omitted from prompts unless the final message is prefilled
  - assistant parts render as individual Harmony messages in their original order
  - the last assistant part is removed from the prompt and used as a prefill when it's text or thinking
  - streaming parser is primed with prefill tokens so continuation in the same channel is captured
  - tool calls render in the commentary channel with constrained JSON
  - tool responses map to tool role messages in the commentary channel addressed to the assistant
- Provider selection
  - `Provider` enum lists supported backends
  - `client_from` builds a client for the given provider and model
    - stores provider and model names for later retrieval
- Tool schemas
  - `to_openapi_schema` strips `$schema` and converts unsigned ints to signed formats
- Core message and tool types defined locally instead of re-exporting from `ollama-rs`
  - tool calls hold name and arguments via `JsonResult`, preserving unparseable argument strings in the `error` variant
  - tool info stores name, description, and parameters without wrapper enums
  - when converting assistant tool calls into provider requests, the function `arguments` include the original tool-call `id` under the `_id` field
- chat messages are an enum of `UserMessage`, `AssistantMessage`, `SystemMessage`, and `ToolMessage`, each with only relevant fields
    - `AssistantMessage` holds a `Vec<AssistantPart>` for text, tool calls, and thinking segments
      - each assistant part carries optional `encrypted_content`; GeminiRust forwards it to and restores it from the Gemini `thought_signature`
    - tool calls include an `id` string, assigned locally when missing
    - tool messages carry the same `id` and store results via `JsonResult` (`content` or `error`)
- Chat message, request, and response types serialize to and from JSON
  - skips serializing fields that are `None`, empty strings, or empty arrays
- Responses
  - `ResponseChunk` emits `Part(AssistantPart)` items alongside `Usage` and `Done`
  - usage chunks carry `input_tokens` and `output_tokens`
  - streaming text and thinking segments arrive as assistant parts; consecutive parts without `encrypted_content` are merged while segments with encrypted data remain isolated
  - tool call parts stream as single `AssistantPart::ToolCall` values
  - OpenAiChat client converts assistant history messages with tool calls into request `tool_calls` and stitches streaming tool call deltas into complete tool calls
  - OpenAiChat client parses `reasoning_content` from streamed responses into thinking text
- Tool orchestration
  - `tools` module exposes a `ToolExecutor` trait
  - `run_tool_loop` streams responses, executes tools, and issues follow-up requests
  - `tool_event_stream` spawns the loop and yields `ToolEvent`s
    - `RequestStarted` fires when a new request is sent
    - join handle resolves on completion with history updated in place
    - `ToolStarted` events carry arguments as `JsonResult` and include original argument strings when parsing fails
    - `ToolStarted` and `ToolResult` keep the original `ToolCall` id
  - tool calls with invalid arguments skip executor invocation and return "Could not parse arguments as JSON"
- `mcp` module
- `load_mcp_servers` starts configured MCP servers and collects tool schemas
  - tool names are prefixed with the server name, joined with an underscore
  - `McpService` implements `ClientHandler`
    - `on_tool_list_changed` refreshes tool metadata from the service
    - tool metadata stored in an `ArcSwap` for lock-free snapshots
  - `McpContext` stores running service handles keyed by prefix
    - supports runtime insertion and removal of services via internal locking
    - exposes merged `tool_infos` from all services
    - provides a non-blocking `tool_names` snapshot of available tools
    - implements `ToolExecutor` for MCP calls
      - unrecognized or unprefixed tool names return "{name} is not a valid tool name"
      - tool execution errors with `is_error` set propagate as `Err`
    - tool orchestration message shape
      - streamed assistant parts (text/thinking) and tool calls from a single response are combined into one assistant message in original order
      - the tool result is appended as a separate tool-role message with the same call id
      - follow-up assistant responses are appended after tool execution completes
- Test utilities
  - `TestProvider` implements `LlmClient`
    - captures `ChatMessageRequest`s for assertions
    - streams queued `ResponseChunk`s for iterative testing

## Constraints
- uses provider-specific default host when none is supplied
- Harmony defaults to `http://localhost:8000` and supports only `gpt-oss` via `/completion`
- deprecated `function_call` streaming is no longer supported
