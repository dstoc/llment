# llm-cli
Basic terminal chat interface scaffold using tuirealm and ratatui.

## Dependencies
- ratatui
  - terminal UI rendering
- tuirealm
  - component-based TUI framework
- textwrap
  - wrap conversation lines
- unicode-width
  - measure display width for proper box padding
- termimad
  - render markdown in assistant responses
- tui-textarea
  - multiline text input with standard editing
- llm
  - send chat messages and handle tool orchestration
- tokio
  - runtime for asynchronous tool streaming
- tokio-stream
  - poll `ToolEvent` updates
- futures
  - utility traits for non-blocking polling

## Features, Requirements and Constraints
- CLI arguments
  - `--provider` selects LLM backend
  - `--model` sets the model identifier
  - `--host` configures the LLM host URL
  - `--mcp` loads MCP server configuration
  - layout
    - scrollable conversation pane
      - mouse wheel adjusts scroll
      - items snap to bottom with blank space above when short
      - auto-scrolls when at bottom or after user sends a message
    - text input field at the bottom
      - supports multi-line editing without wrapping
      - height expands to fit content
      - Enter submits the message
      - Ctrl-J inserts a new line
      - standard shortcuts: Ctrl-W delete previous word, Ctrl-L clears input
      - paste inserts clipboard text
      - clicking the field focuses it
      - cursor hidden when unfocused
      - trailing spaces do not move the cursor to the next line
    - Esc exits the application
    - conversation pane has no keyboard interaction
    - conversation items
      - initialized with empty history
      - user messages render inside a right-aligned rounded block
      - assistant messages show working steps and final response
        - working and tool sections toggle with mouse click
        - final responses render markdown via termimad
        - streaming updates append thinking text, tool calls, and tool results
        - header summarizes thinking duration and completed tool usage
    - items stored as a strongly typed `Node` enum implementing `ConvNode`
      - helper methods append items and steps, bumping `content_rev` for caching
    - partial items are clipped when scrolled
    - line caches invalidate on width or content changes
    - clicking items toggles collapse without selection
  - code structure
    - conversation resides under `src/conversation` with modules for nodes and mutation helpers
  - tool streaming
    - drains remaining events after request completes before clearing state
  - MCP integration
  - `ChatMessageRequest` includes MCP `tool_infos` before enabling thinking
